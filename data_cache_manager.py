#!/usr/bin/env python3
"""
æœ¬åœ°æ•°æ®ç¼“å­˜ç®¡ç†å™¨
æŒä¹…åŒ–ä¿å­˜å†å²Kçº¿æ•°æ®ï¼Œæ”¯æŒå¿«é€Ÿå›æµ‹

åŠŸèƒ½ï¼š
1. è‡ªåŠ¨ä¿å­˜æ‹‰å–çš„å†å²æ•°æ®åˆ°æœ¬åœ°
2. ä¼˜å…ˆä»æœ¬åœ°è¯»å–ï¼Œç¼ºå¤±æ—¶æ‰ä»APIæ‹‰å–
3. æ”¯æŒæ•°æ®æ›´æ–°ï¼ˆè¿½åŠ æœ€æ–°æ•°æ®ï¼‰
4. æ”¯æŒæ•°æ®ç®¡ç†ï¼ˆæŸ¥çœ‹ã€æ¸…ç†ã€å¯¼å‡ºï¼‰

æ•°æ®ç»“æ„ï¼š
data/cache/
  â”œâ”€â”€ BTC_USDT_1h.csv
  â”œâ”€â”€ BTC_USDT_30m.csv
  â”œâ”€â”€ ETH_USDT_1h.csv
  â””â”€â”€ ...

æ¯ä¸ªæ–‡ä»¶åŒ…å«ï¼štimestamp, open, high, low, close, volume
"""

import os
import pandas as pd
from pathlib import Path
from datetime import datetime, timedelta
from typing import Optional
import logging

logger = logging.getLogger(__name__)


class DataCacheManager:
    """æœ¬åœ°æ•°æ®ç¼“å­˜ç®¡ç†å™¨"""

    def __init__(self, cache_dir: str = 'data/cache'):
        """
        åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨

        Args:
            cache_dir: ç¼“å­˜ç›®å½•è·¯å¾„
        """
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"âœ… æ•°æ®ç¼“å­˜ç›®å½•: {self.cache_dir.absolute()}")

    def _get_cache_path(self, symbol: str, timeframe: str) -> Path:
        """
        è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„

        Args:
            symbol: äº¤æ˜“å¯¹ï¼Œå¦‚ 'BTC/USDT'
            timeframe: æ—¶é—´å‘¨æœŸï¼Œå¦‚ '1h'

        Returns:
            ç¼“å­˜æ–‡ä»¶è·¯å¾„
        """
        # å°† BTC/USDT è½¬æ¢ä¸º BTC_USDT
        safe_symbol = symbol.replace('/', '_')
        filename = f"{safe_symbol}_{timeframe}.csv"
        return self.cache_dir / filename

    def load_from_cache(self, symbol: str, timeframe: str) -> Optional[pd.DataFrame]:
        """
        ä»ç¼“å­˜åŠ è½½æ•°æ®

        Args:
            symbol: äº¤æ˜“å¯¹
            timeframe: æ—¶é—´å‘¨æœŸ

        Returns:
            DataFrameæˆ–None
        """
        cache_path = self._get_cache_path(symbol, timeframe)

        if not cache_path.exists():
            logger.info(f"âš ï¸  ç¼“å­˜ä¸å­˜åœ¨: {cache_path.name}")
            return None

        try:
            df = pd.read_csv(cache_path, index_col=0, parse_dates=True)
            logger.info(f"âœ… ä»ç¼“å­˜åŠ è½½: {cache_path.name}")
            logger.info(f"   æ•°æ®èŒƒå›´: {df.index[0]} è‡³ {df.index[-1]}")
            logger.info(f"   æ•°æ®æ¡æ•°: {len(df)}")
            return df
        except Exception as e:
            logger.error(f"âŒ åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
            return None

    def save_to_cache(self, df: pd.DataFrame, symbol: str, timeframe: str):
        """
        ä¿å­˜æ•°æ®åˆ°ç¼“å­˜

        Args:
            df: æ•°æ®DataFrame
            symbol: äº¤æ˜“å¯¹
            timeframe: æ—¶é—´å‘¨æœŸ
        """
        cache_path = self._get_cache_path(symbol, timeframe)

        try:
            df.to_csv(cache_path)
            logger.info(f"ğŸ’¾ ä¿å­˜åˆ°ç¼“å­˜: {cache_path.name}")
            logger.info(f"   æ•°æ®èŒƒå›´: {df.index[0]} è‡³ {df.index[-1]}")
            logger.info(f"   æ•°æ®æ¡æ•°: {len(df)}")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")

    def merge_and_save(self, new_df: pd.DataFrame, symbol: str, timeframe: str) -> pd.DataFrame:
        """
        åˆå¹¶æ–°æ—§æ•°æ®å¹¶ä¿å­˜

        Args:
            new_df: æ–°æ‹‰å–çš„æ•°æ®
            symbol: äº¤æ˜“å¯¹
            timeframe: æ—¶é—´å‘¨æœŸ

        Returns:
            åˆå¹¶åçš„å®Œæ•´æ•°æ®
        """
        # åŠ è½½ç°æœ‰ç¼“å­˜
        cached_df = self.load_from_cache(symbol, timeframe)

        if cached_df is None:
            # æ²¡æœ‰ç¼“å­˜ï¼Œç›´æ¥ä¿å­˜
            self.save_to_cache(new_df, symbol, timeframe)
            return new_df

        # åˆå¹¶æ•°æ®
        merged_df = pd.concat([cached_df, new_df])

        # å»é‡ï¼ˆæŒ‰æ—¶é—´æˆ³ï¼‰
        merged_df = merged_df[~merged_df.index.duplicated(keep='last')]

        # æ’åº
        merged_df = merged_df.sort_index()

        # ä¿å­˜
        self.save_to_cache(merged_df, symbol, timeframe)

        logger.info(f"ğŸ”„ æ•°æ®åˆå¹¶å®Œæˆ:")
        logger.info(f"   åŸæœ‰: {len(cached_df)} æ¡")
        logger.info(f"   æ–°å¢: {len(new_df)} æ¡")
        logger.info(f"   æ€»è®¡: {len(merged_df)} æ¡")

        return merged_df

    def update_latest(self, symbol: str, timeframe: str, data_collector) -> pd.DataFrame:
        """
        æ›´æ–°æœ€æ–°æ•°æ®

        Args:
            symbol: äº¤æ˜“å¯¹
            timeframe: æ—¶é—´å‘¨æœŸ
            data_collector: DataCollectorå®ä¾‹

        Returns:
            æ›´æ–°åçš„å®Œæ•´æ•°æ®
        """
        cached_df = self.load_from_cache(symbol, timeframe)

        if cached_df is None:
            # æ²¡æœ‰ç¼“å­˜ï¼Œæ‹‰å–å®Œæ•´æ•°æ®
            logger.info("ğŸ“¥ é¦–æ¬¡æ‹‰å–ï¼Œè·å–å®Œæ•´å†å²æ•°æ®...")
            new_df = data_collector.fetch_ohlcv(symbol, timeframe, limit=1000)
            self.save_to_cache(new_df, symbol, timeframe)
            return new_df

        # è®¡ç®—éœ€è¦æ›´æ–°çš„æ•°æ®é‡
        last_time = cached_df.index[-1]
        now = pd.Timestamp.now(tz='UTC')
        time_diff = now - last_time

        # æ ¹æ®æ—¶é—´å‘¨æœŸè®¡ç®—éœ€è¦æ‹‰å–çš„Kçº¿æ•°é‡
        timeframe_minutes = {
            '1m': 1, '5m': 5, '15m': 15, '30m': 30,
            '1h': 60, '4h': 240, '1d': 1440
        }
        minutes = timeframe_minutes.get(timeframe, 60)
        bars_needed = int(time_diff.total_seconds() / 60 / minutes) + 10  # +10ç¡®ä¿è¦†ç›–

        if bars_needed <= 0:
            logger.info("âœ… æ•°æ®å·²æ˜¯æœ€æ–°ï¼Œæ— éœ€æ›´æ–°")
            return cached_df

        logger.info(f"ğŸ“¥ æ›´æ–°æœ€æ–°æ•°æ®ï¼Œé¢„è®¡éœ€è¦ {bars_needed} æ ¹Kçº¿...")
        new_df = data_collector.fetch_ohlcv(symbol, timeframe, limit=min(bars_needed, 1000))

        # åˆå¹¶å¹¶ä¿å­˜
        merged_df = self.merge_and_save(new_df, symbol, timeframe)
        return merged_df

    def get_stats(self) -> dict:
        """
        è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        stats = {
            'total_files': 0,
            'total_size_mb': 0,
            'files': []
        }

        for cache_file in self.cache_dir.glob('*.csv'):
            try:
                df = pd.read_csv(cache_file, index_col=0, parse_dates=True)
                size_mb = cache_file.stat().st_size / (1024 * 1024)

                stats['files'].append({
                    'name': cache_file.name,
                    'rows': len(df),
                    'size_mb': size_mb,
                    'start': df.index[0],
                    'end': df.index[-1],
                    'days': (df.index[-1] - df.index[0]).days
                })

                stats['total_files'] += 1
                stats['total_size_mb'] += size_mb
            except Exception as e:
                logger.warning(f"âš ï¸  æ— æ³•è¯»å– {cache_file.name}: {e}")

        return stats

    def print_stats(self):
        """æ‰“å°ç¼“å­˜ç»Ÿè®¡"""
        stats = self.get_stats()

        print(f"\n{'='*80}")
        print(f"ğŸ“Š æœ¬åœ°æ•°æ®ç¼“å­˜ç»Ÿè®¡")
        print(f"{'='*80}")
        print(f"ç¼“å­˜ç›®å½•: {self.cache_dir.absolute()}")
        print(f"æ–‡ä»¶æ€»æ•°: {stats['total_files']}")
        print(f"æ€»å¤§å°:   {stats['total_size_mb']:.2f} MB")
        print()

        if stats['files']:
            print(f"{'æ–‡ä»¶å':<25} {'æ•°æ®æ¡æ•°':<10} {'å¤©æ•°':<8} {'å¤§å°':<10} {'æ—¶é—´èŒƒå›´'}")
            print(f"{'-'*80}")
            for file_info in sorted(stats['files'], key=lambda x: x['name']):
                print(f"{file_info['name']:<25} "
                      f"{file_info['rows']:<10} "
                      f"{file_info['days']:<8} "
                      f"{file_info['size_mb']:.2f} MB   "
                      f"{file_info['start']} ~ {file_info['end']}")
        else:
            print("âš ï¸  ç¼“å­˜ä¸ºç©º")

        print(f"{'='*80}\n")

    def clear_cache(self, symbol: Optional[str] = None, timeframe: Optional[str] = None):
        """
        æ¸…ç†ç¼“å­˜

        Args:
            symbol: äº¤æ˜“å¯¹ï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨ï¼‰
            timeframe: æ—¶é—´å‘¨æœŸï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨ï¼‰
        """
        if symbol and timeframe:
            # æ¸…ç†ç‰¹å®šæ–‡ä»¶
            cache_path = self._get_cache_path(symbol, timeframe)
            if cache_path.exists():
                cache_path.unlink()
                logger.info(f"ğŸ—‘ï¸  å·²åˆ é™¤: {cache_path.name}")
            else:
                logger.warning(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {cache_path.name}")
        else:
            # æ¸…ç†æ‰€æœ‰æ–‡ä»¶
            count = 0
            for cache_file in self.cache_dir.glob('*.csv'):
                cache_file.unlink()
                count += 1
            logger.info(f"ğŸ—‘ï¸  å·²æ¸…ç† {count} ä¸ªç¼“å­˜æ–‡ä»¶")


# ==================== å‘½ä»¤è¡Œå·¥å…· ====================
def main():
    """ä¸»å‡½æ•°ï¼šç¼“å­˜ç®¡ç†å·¥å…·"""
    import argparse

    parser = argparse.ArgumentParser(description='æ•°æ®ç¼“å­˜ç®¡ç†å·¥å…·')
    parser.add_argument('action', choices=['stats', 'update', 'clear'],
                        help='æ“ä½œï¼šstats(ç»Ÿè®¡), update(æ›´æ–°), clear(æ¸…ç†)')
    parser.add_argument('--symbol', help='äº¤æ˜“å¯¹ï¼Œå¦‚ BTC/USDT')
    parser.add_argument('--timeframe', '-t', help='æ—¶é—´å‘¨æœŸï¼Œå¦‚ 1h')
    parser.add_argument('--all', action='store_true', help='æ›´æ–°æ‰€æœ‰äº¤æ˜“å¯¹')

    args = parser.parse_args()

    manager = DataCacheManager()

    if args.action == 'stats':
        # æ˜¾ç¤ºç»Ÿè®¡
        manager.print_stats()

    elif args.action == 'update':
        # æ›´æ–°æ•°æ®
        from data_collector import DataCollector
        from config.strategy_params import TRADING_SYMBOLS

        collector = DataCollector('binance')

        if args.all:
            # æ›´æ–°æ‰€æœ‰äº¤æ˜“å¯¹
            timeframes = ['1h', '30m', '15m']
            for symbol in TRADING_SYMBOLS:
                for tf in timeframes:
                    print(f"\næ›´æ–° {symbol} @ {tf}...")
                    manager.update_latest(symbol, tf, collector)
        elif args.symbol and args.timeframe:
            # æ›´æ–°æŒ‡å®šäº¤æ˜“å¯¹
            manager.update_latest(args.symbol, args.timeframe, collector)
        else:
            print("âŒ è¯·æŒ‡å®š --symbol å’Œ --timeframeï¼Œæˆ–ä½¿ç”¨ --all")

    elif args.action == 'clear':
        # æ¸…ç†ç¼“å­˜
        if args.symbol and args.timeframe:
            manager.clear_cache(args.symbol, args.timeframe)
        else:
            confirm = input("âš ï¸  ç¡®è®¤æ¸…ç†æ‰€æœ‰ç¼“å­˜ï¼Ÿ(y/n): ")
            if confirm.lower() == 'y':
                manager.clear_cache()


if __name__ == '__main__':
    logging.basicConfig(level=logging.INFO)
    main()
